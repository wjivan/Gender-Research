{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('zeina': conda)",
   "metadata": {
    "interpreter": {
     "hash": "23145376f35b1170ea1bf4b28bc7e8e771587d1d28226557d53e8705cb8082c2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ----------- UTILITY ---------------\n",
    "def clean_string(string):\n",
    "    string = unicodedata.normalize('NFKD',string) \\\n",
    "        .encode('ascii', 'ignore') \\\n",
    "        .decode('ascii') \\\n",
    "        .lower() \\\n",
    "        .strip() \\\n",
    "        .title()\n",
    "    return string\n",
    "\n",
    "def clean_series(series):\n",
    "    cleaned = series.map(clean_string)\n",
    "    return cleaned\n",
    "\n",
    "def standardise_column_names(df, remove_punct=True):\n",
    "    \"\"\" Converts all DataFrame column names to lower case replacing\n",
    "    whitespace of any length with a single underscore. Can also strip\n",
    "    all punctuation from column names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame with non-standardised column names.\n",
    "    remove_punct: bool (default True)\n",
    "        If True will remove all punctuation from column names.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame with standardised column names.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "    for c in df.columns:\n",
    "        c_mod = c.lower()\n",
    "        if remove_punct:            \n",
    "            c_mod = c_mod.translate(translator)\n",
    "        c_mod = '_'.join(c_mod.split(' '))\n",
    "        if c_mod[-1] == '_':\n",
    "            c_mod = c_mod[:-1]\n",
    "        c_mod = re.sub(r'\\_+', '_', c_mod)\n",
    "        df.rename({c: c_mod}, inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "# PIPELINE ----------------->\n",
    "# Set up soup\n",
    "def setup_soup(url):\n",
    "    # Setup beautiful soup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Scrape papers information from the author\n",
    "def scrape_papers(soup):\n",
    "    # Publication classifications free, gate, none\n",
    "    # There will be overlaps\n",
    "    publications = soup.find_all('li', class_={'list-group-item downfree', \\\n",
    "        'list-group-item downgate', 'list-group-item downnone'})\n",
    "    paper_details = {}\n",
    "\n",
    "    i = 1\n",
    "    for pub in publications:\n",
    "        try:\n",
    "            title = pub.find('a').text\n",
    "            # Get paper url and do some cleaning\n",
    "            paper_url = pub.find('a')['href'].replace('https://ideas.repec.org/','')\n",
    "            if not paper_url[0] == '/':\n",
    "                paper_url = '/'+paper_url\n",
    "            \n",
    "            # Get the authors and year of paper\n",
    "            name_year = pub.text.strip().split('\\n')[0]\n",
    "            if 'undated' in name_year:\n",
    "                year = None\n",
    "                authors = re.sub(r', \\\"undated\\\"', '',name_year).split(' & ')\n",
    "            else:\n",
    "                year = int(re.findall(r', (\\d{4})\\.', name_year)[0])\n",
    "                authors = re.sub(r', \\d{4}\\.', '',name_year).split(' & ')\n",
    "            paper_details[title] = {'author': authors, 'year': year, 'paper_url': paper_url}\n",
    "        except:\n",
    "            print('something went wrong at paper {}'.format(i))\n",
    "        i +=1\n",
    "    return paper_details\n",
    "\n",
    "# Scraping personal information of the author\n",
    "def scrape_personal(soup):\n",
    "    # Find portion where personal details lie in\n",
    "    personal_details = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    # Set up a dictionary to collect all personal information\n",
    "    per = {}\n",
    "    for p in personal_details:\n",
    "        k = p.find_all('td')[0].text.replace(':','')\n",
    "        v = p.find_all('td')[1].text.strip()\n",
    "        per[k] = v\n",
    "    \n",
    "    per_clean = {k:v for (k,v) in per.items() if (v is not '') }\n",
    "    \n",
    "\n",
    "    # Find homepage link\n",
    "    try:    \n",
    "        homepage = soup.find('td', {'class':'homelabel'}).next_sibling.find('a', href=True)['href']\n",
    "        per_clean['Homepage'] = homepage\n",
    "    except:\n",
    "        print('homepage not found')\n",
    "\n",
    "    # Find affiliation - can have multiple\n",
    "    affiliation_soup = soup.find('div', {'id':'affiliation'})\n",
    "\n",
    "    i = 0\n",
    "    try:\n",
    "        for a in affiliation_soup.find_all('h3'):\n",
    "            if a.find('br'):\n",
    "                department = a.find('br').previous_sibling\n",
    "                organisation = a.find('br').next_sibling\n",
    "            else:\n",
    "                print('no breaks in affiliation')\n",
    "                department = ''\n",
    "                organisation = a\n",
    "            per_clean['Aff_Department{}'.format(i)] = department\n",
    "            per_clean['Aff_Organisation{}'.format(i)] = organisation\n",
    "            i += 1\n",
    "    except:\n",
    "        print('affiliation not found')\n",
    "\n",
    "    # Find affiliation locations - can have multiple\n",
    "    i = 0\n",
    "    try:\n",
    "        for a in affiliation_soup.find_all('span', {'class':'locationlabel'}):\n",
    "            if a:\n",
    "                location = a.text\n",
    "            else:\n",
    "                print('no location in affiliation')\n",
    "            per_clean['Aff_Location{}'.format(i)] = location\n",
    "            i += 1\n",
    "    except:\n",
    "        print('affiliation not found')\n",
    "\n",
    "    # Drop unnamed items\n",
    "    per_clean = {k:v for (k,v) in per_clean.items() if (k is not '') }\n",
    "\n",
    "    return per_clean\n",
    "\n",
    "# Flatten the paper details into a dataframe to be inserted into database\n",
    "def makedf_paper(paper_details):\n",
    "    # Flatten the paper_details dictionary into a pandas dataframe\n",
    "    pd_paperdetails = pd.DataFrame(paper_details) \\\n",
    "        .transpose() \\\n",
    "        .explode('author') \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns = {'index':'paper'})\n",
    "    \n",
    "    # Make capitalise titles\n",
    "    pd_paperdetails[['paper','author']] = pd_paperdetails[['paper','author']] \\\n",
    "        .apply(clean_series, axis=1)\n",
    "\n",
    "    # Drop duplicates\n",
    "    pd_paperdetails = pd_paperdetails.drop_duplicates(\n",
    "        subset = ['paper_url', 'author'])\n",
    "    \n",
    "    # Drop titles that are very similar \n",
    "    similar = process.dedupe(list(pd_paperdetails['paper'].unique()), threshold = 95)\n",
    "    pd_paperdetails = pd_paperdetails[pd_paperdetails['paper'].isin(similar)]\n",
    "   \n",
    "    # Cleaning\n",
    "    # Replace anything like (Ed.)\n",
    "    pd_paperdetails['author'] = pd_paperdetails['author'].str.replace(r'\\(.*\\)', '')\n",
    "    # Make year numeric\n",
    "    pd_paperdetails['year'] = pd.to_numeric(pd_paperdetails['year']) \n",
    "\n",
    "    # There are instances where author name appears as Lastname, Firstname\n",
    "    def reverse_comma(x):\n",
    "        if ',' in x:\n",
    "            x = x.split(', ')[-1] +' '+ x.split(', ')[0]\n",
    "        return x\n",
    "\n",
    "    pd_paperdetails['author'] = pd_paperdetails['author'].map(reverse_comma)\n",
    "\n",
    "    # Create first & last name & limit to 2 splits - first name, middle name, last name\n",
    "    pd_paperdetails['first_name'] = pd_paperdetails['author'].str.split(None, 2).str[0]\n",
    "    pd_paperdetails['last_name'] = pd_paperdetails['author'].str.split(None, 2).str[-1]\n",
    "    return pd_paperdetails\n",
    "\n",
    "def makedf_personal(personal_details):\n",
    "    # Make DF\n",
    "    df_personal = pd.DataFrame.from_records([personal_details])\n",
    "    # Standardise column names\n",
    "    df_personal = standardise_column_names(df_personal)\n",
    "    return df_personal\n",
    "\n",
    "def reconcile_first_name(df_paper, df_personal):\n",
    "    df_paper = df_paper.merge(df_personal[['first_name','last_name']], on=['last_name'], how='left')\n",
    "    df_paper['first_name'] = np.where(df_paper['first_name_y'].notnull(), df_paper['first_name_y'], df_paper['first_name_x'])\n",
    "    df_paper = df_paper.drop(['first_name_x', 'first_name_y'], axis=1)\n",
    "    df_paper = df_paper.drop_duplicates(subset=['paper','first_name','last_name'])\n",
    "    return df_paper\n",
    "\n",
    "def attach_abstract(df_paper):\n",
    "    paper_urls = df_paper['paper_url'].drop_duplicates()\n",
    "    abstract_dict = {}\n",
    "    for a in paper_urls:\n",
    "        try:\n",
    "            soup =  setup_soup('https://ideas.repec.org' + a)\n",
    "            abstract_text = soup.find('div', {'id':'abstract-body'}).text\n",
    "            abstract_dict[a] = abstract_text\n",
    "        except:\n",
    "            print('Paper {} cannot find abstract'.format(a))\n",
    "            abstract_dict[a] = None\n",
    "    abstract_table = pd.DataFrame([(x,y) for x,y in abstract_dict.items()], columns = ['paper_url','abstract'])\n",
    "    df_paper = df_paper.merge(abstract_table, on='paper_url', how='left')\n",
    "\n",
    "    # Remove potential duplicates\n",
    "    abstract_table = abstract_table.drop_duplicates(subset=['abstract'])\n",
    "    df_paper = df_paper[df_paper['paper_url'].isin(list(abstract_table['paper_url'].unique()))]\n",
    "    return df_paper\n",
    "\n",
    "\n",
    "# url = 'https://ideas.repec.org/e/pag127.html'\n",
    "# soup = setup_soup(url)\n",
    "# paper_details = scrape_papers(soup)\n",
    "# personal_details = scrape_personal(soup)\n",
    "# df_paper = makedf_paper(paper_details)\n",
    "# df_personal = makedf_personal(personal_details)\n",
    "# df_paper = reconcile_first_name(df_paper, df_personal)\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "homepage not found\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ideas.repec.org/e/pag127.html'\n",
    "soup = setup_soup(url)\n",
    "paper_details = scrape_papers(soup)\n",
    "personal_details = scrape_personal(soup)\n",
    "df_paper = makedf_paper(paper_details)\n",
    "df_personal = makedf_personal(personal_details)\n",
    "df_paper = reconcile_first_name(df_paper, df_personal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "x[0] == '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Paper /p/bdi/wptemi/td_1209_19.html cannot find abstract\n",
      "Paper /a/oup/cesifo/v62y2016i1p126-147..html cannot find abstract\n"
     ]
    }
   ],
   "source": [
    "paper_urls = df_paper['paper_url'].drop_duplicates()\n",
    "abstract_dict = {}\n",
    "for a in paper_urls:\n",
    "    try:\n",
    "        soup =  setup_soup('https://ideas.repec.org' + a)\n",
    "        abstract_text = soup.find('div', {'id':'abstract-body'}).text\n",
    "        abstract_dict[a] = abstract_text\n",
    "    except:\n",
    "        print('Paper {} cannot find abstract'.format(a))\n",
    "        abstract_dict[a] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame([(x,y) for x,y in abstract_dict.items()], columns = ['paper_url','abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                paper_url  \\\n",
       "0          /p/hig/wpaper/226-ec-2020.html   \n",
       "1                   /p/ipu/wpaper/90.html   \n",
       "2                   /p/ipu/wpaper/99.html   \n",
       "3               /p/uto/dipeco/202022.html   \n",
       "4          /p/hig/wpaper/224-ec-2020.html   \n",
       "..                                    ...   \n",
       "73  /a/taf/edecon/v15y2007i4p455-471.html   \n",
       "74   /a/ids/ijbpma/v8y2006i4p344-367.html   \n",
       "75             /h/elg/eechap/17209_9.html   \n",
       "76               /h/aec/ieed06/06-31.html   \n",
       "77               /b/elg/eebook/17209.html   \n",
       "\n",
       "                                             abstract  \n",
       "0   This paper studies the effect of merger polici...  \n",
       "1   In Italy, the provision of educational ancilla...  \n",
       "2   This paper contributes to the empirical litera...  \n",
       "3   This paper contributes to the empirical litera...  \n",
       "4   This paper studies the relationship between un...  \n",
       "..                                                ...  \n",
       "73   This study uses Data Envelopment Analysis to ...  \n",
       "74   In this paper, we consider the problem of det...  \n",
       "75  In this chapter, we outline the importance of ...  \n",
       "76  The Italian educational system is strictly reg...  \n",
       "77  This Handbook provides a comprehensive overvie...  \n",
       "\n",
       "[78 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper_url</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/p/hig/wpaper/226-ec-2020.html</td>\n      <td>This paper studies the effect of merger polici...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/p/ipu/wpaper/90.html</td>\n      <td>In Italy, the provision of educational ancilla...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/p/ipu/wpaper/99.html</td>\n      <td>This paper contributes to the empirical litera...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/p/uto/dipeco/202022.html</td>\n      <td>This paper contributes to the empirical litera...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/p/hig/wpaper/224-ec-2020.html</td>\n      <td>This paper studies the relationship between un...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>/a/taf/edecon/v15y2007i4p455-471.html</td>\n      <td>This study uses Data Envelopment Analysis to ...</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>/a/ids/ijbpma/v8y2006i4p344-367.html</td>\n      <td>In this paper, we consider the problem of det...</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>/h/elg/eechap/17209_9.html</td>\n      <td>In this chapter, we outline the importance of ...</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>/h/aec/ieed06/06-31.html</td>\n      <td>The Italian educational system is strictly reg...</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>/b/elg/eebook/17209.html</td>\n      <td>This Handbook provides a comprehensive overvie...</td>\n    </tr>\n  </tbody>\n</table>\n<p>78 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_abstract(df_paper):\n",
    "    paper_urls = df_paper['paper_url'].drop_duplicates()\n",
    "    abstract_dict = {}\n",
    "    for a in paper_urls:\n",
    "        try:\n",
    "            soup =  setup_soup('https://ideas.repec.org' + a)\n",
    "            abstract_text = soup.find('div', {'id':'abstract-body'}).text\n",
    "            abstract_dict[a] = abstract_text\n",
    "        except:\n",
    "            print('Paper {} cannot find abstract'.format(a))\n",
    "            abstract_dict[a] = None\n",
    "    abstract_table = pd.DataFrame([(x,y) for x,y in abstract_dict.items()], columns = ['paper_url','abstract'])\n",
    "    df_paper = df_paper.merge(abstract_table, on='paper_url', how='left')\n",
    "    df_paper.drop_duplicates(subset=['abstract','first_name','last_name'])\n",
    "    return df_paper\n"
   ]
  }
 ]
}