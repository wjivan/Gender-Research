{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('zeina': conda)",
   "metadata": {
    "interpreter": {
     "hash": "23145376f35b1170ea1bf4b28bc7e8e771587d1d28226557d53e8705cb8082c2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------- UTILITY ---------------\n",
    "def clean_string(string):\n",
    "    string = unicodedata.normalize('NFKD',string) \\\n",
    "        .encode('ascii', 'ignore') \\\n",
    "        .decode('ascii') \\\n",
    "        .lower() \\\n",
    "        .strip() \\\n",
    "        .title()\n",
    "    return string\n",
    "\n",
    "def clean_series(series):\n",
    "    cleaned = series.map(clean_string)\n",
    "    return cleaned\n",
    "\n",
    "def standardise_column_names(df, remove_punct=True):\n",
    "    \"\"\" Converts all DataFrame column names to lower case replacing\n",
    "    whitespace of any length with a single underscore. Can also strip\n",
    "    all punctuation from column names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame with non-standardised column names.\n",
    "    remove_punct: bool (default True)\n",
    "        If True will remove all punctuation from column names.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame with standardised column names.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "    for c in df.columns:\n",
    "        c_mod = c.lower()\n",
    "        if remove_punct:            \n",
    "            c_mod = c_mod.translate(translator)\n",
    "        c_mod = '_'.join(c_mod.split(' '))\n",
    "        if c_mod[-1] == '_':\n",
    "            c_mod = c_mod[:-1]\n",
    "        c_mod = re.sub(r'\\_+', '_', c_mod)\n",
    "        df.rename({c: c_mod}, inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "# There are instances where author name appears as Lastname, Firstname\n",
    "def reverse_comma(x):\n",
    "    if ',' in x:\n",
    "        x = x.split(', ')[-1] +' '+ x.split(', ')[0]\n",
    "    return x\n",
    "\n",
    "# PIPELINE ----------------->\n",
    "# Set up soup\n",
    "def setup_soup(url):\n",
    "    # Setup beautiful soup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Obtain a list of urls to scrape\n",
    "def get_author_urls():\n",
    "    url = 'https://ideas.repec.org/i/eall.html'\n",
    "    soup = setup_soup(url)\n",
    "    links = soup.find_all('a', href=True)\n",
    "    url_collection = []\n",
    "    for i in tqdm(links,desc='Downloading links'):\n",
    "        author = i.text\n",
    "        author_url = i['href']\n",
    "        url_collection.append([author, author_url]) \n",
    "    \n",
    "    # Get rid of unnecessary links by finding the position of the first and last author\n",
    "    pos = [idx for idx, results in enumerate(url_collection) if ('Aaberge, Rolf ' in results[0]) or ('Zhou, Li ' in results[0])]\n",
    "    url_collection = url_collection[min(pos):max(pos)+1]    \n",
    "\n",
    "    # Some cleaning and get it into a DF\n",
    "    clean_urls = [reverse_comma(clean_string(u[0])).split(None, 2) + [u[1]] for u in url_collection]\n",
    "    cleaned = pd.DataFrame(clean_urls, columns=['first_name','middle_name','last_name','partial'])\n",
    "    cleaned['author_url'] = np.where(cleaned['partial'].isnull(), cleaned['last_name'], cleaned['partial'])\n",
    "    cleaned['last_name'] = np.where(cleaned['partial'].isnull(), cleaned['middle_name'], cleaned['last_name'])\n",
    "    cleaned['middle_name'] = np.where(cleaned['partial'].isnull(), None, cleaned['middle_name'])\n",
    "    cleaned = cleaned.drop(columns=['partial'])\n",
    "    return cleaned\n",
    "\n",
    "# Scrape papers information from the author\n",
    "def scrape_papers(soup):\n",
    "    # Publication classifications free, gate, none\n",
    "    # There will be overlaps\n",
    "    publications = soup.find_all('li', class_={'list-group-item downfree', \\\n",
    "        'list-group-item downgate', 'list-group-item downnone'})\n",
    "    paper_details = {}\n",
    "\n",
    "    i = 1\n",
    "    for pub in publications:\n",
    "        try:\n",
    "            title = pub.find('a').text\n",
    "            # Get paper url and do some cleaning\n",
    "            paper_url = pub.find('a')['href'].replace('https://ideas.repec.org/','')\n",
    "            if not paper_url[0] == '/':\n",
    "                paper_url = '/'+paper_url\n",
    "            \n",
    "            # Get the authors and year of paper\n",
    "            name_year = pub.text.strip().split('\\n')[0]\n",
    "            if 'undated' in name_year:\n",
    "                year = None\n",
    "                authors = re.sub(r', \\\"undated\\\"', '',name_year).split(' & ')\n",
    "            else:\n",
    "                year = int(re.findall(r', (\\d{4})\\.', name_year)[0])\n",
    "                authors = re.sub(r', \\d{4}\\.', '',name_year).split(' & ')\n",
    "            paper_details[title] = {'author': authors, 'year': year, 'paper_url': paper_url}\n",
    "        except:\n",
    "            print('something went wrong at paper {}'.format(i))\n",
    "        i +=1\n",
    "    return paper_details\n",
    "\n",
    "# Scraping personal information of the author\n",
    "def scrape_personal(soup):\n",
    "    # Find portion where personal details lie in\n",
    "    personal_details = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    # Set up a dictionary to collect all personal information\n",
    "    per = {}\n",
    "    for p in personal_details:\n",
    "        k = p.find_all('td')[0].text.replace(':','')\n",
    "        v = p.find_all('td')[1].text.strip()\n",
    "        per[k] = v\n",
    "    \n",
    "    per_clean = {k:v for (k,v) in per.items() if (v is not '') }\n",
    "    \n",
    "\n",
    "    # Find homepage link\n",
    "    try:    \n",
    "        homepage = soup.find('td', {'class':'homelabel'}).next_sibling.find('a', href=True)['href']\n",
    "        per_clean['Homepage'] = homepage\n",
    "    except:\n",
    "        print('homepage not found')\n",
    "\n",
    "    # Find affiliation - can have multiple\n",
    "    affiliation_soup = soup.find('div', {'id':'affiliation'})\n",
    "\n",
    "    i = 0\n",
    "    try:\n",
    "        for a in affiliation_soup.find_all('h3'):\n",
    "            if a.find('br'):\n",
    "                department = a.find('br').previous_sibling\n",
    "                organisation = a.find('br').next_sibling\n",
    "            else:\n",
    "                print('no breaks in affiliation')\n",
    "                department = ''\n",
    "                organisation = a\n",
    "            per_clean['Aff_Department{}'.format(i)] = department\n",
    "            per_clean['Aff_Organisation{}'.format(i)] = organisation\n",
    "            i += 1\n",
    "    except:\n",
    "        print('affiliation not found')\n",
    "\n",
    "    # Find affiliation locations - can have multiple\n",
    "    i = 0\n",
    "    try:\n",
    "        for a in affiliation_soup.find_all('span', {'class':'locationlabel'}):\n",
    "            if a:\n",
    "                location = a.text\n",
    "            else:\n",
    "                print('no location in affiliation')\n",
    "            per_clean['Aff_Location{}'.format(i)] = location\n",
    "            i += 1\n",
    "    except:\n",
    "        print('affiliation not found')\n",
    "\n",
    "    # Drop unnamed items\n",
    "    per_clean = {k:v for (k,v) in per_clean.items() if (k is not '') }\n",
    "\n",
    "    return per_clean\n",
    "\n",
    "# Flatten the paper details into a dataframe to be inserted into database\n",
    "def makedf_paper(paper_details):\n",
    "    # Flatten the paper_details dictionary into a pandas dataframe\n",
    "    pd_paperdetails = pd.DataFrame(paper_details) \\\n",
    "        .transpose() \\\n",
    "        .explode('author') \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns = {'index':'paper'})\n",
    "    \n",
    "    # Make capitalise titles\n",
    "    pd_paperdetails[['paper','author']] = pd_paperdetails[['paper','author']] \\\n",
    "        .apply(clean_series, axis=1)\n",
    "\n",
    "    # Drop duplicates\n",
    "    pd_paperdetails = pd_paperdetails.drop_duplicates(\n",
    "        subset = ['paper_url', 'author'])\n",
    "    \n",
    "    # Drop titles that are very similar \n",
    "    similar = process.dedupe(list(pd_paperdetails['paper'].unique()), threshold = 95)\n",
    "    pd_paperdetails = pd_paperdetails[pd_paperdetails['paper'].isin(similar)]\n",
    "   \n",
    "    # Cleaning\n",
    "    # Replace anything like (Ed.)\n",
    "    pd_paperdetails['author'] = pd_paperdetails['author'].str.replace(r'\\(.*\\)', '')\n",
    "    # Make year numeric\n",
    "    pd_paperdetails['year'] = pd.to_numeric(pd_paperdetails['year']) \n",
    "    \n",
    "    # Reverse Firstname last name\n",
    "    pd_paperdetails['author'] = pd_paperdetails['author'].map(reverse_comma)\n",
    "\n",
    "    # Create first & last name & limit to 2 splits - first name, middle name, last name\n",
    "    pd_paperdetails['first_name'] = pd_paperdetails['author'].str.split(None, 2).str[0]\n",
    "    pd_paperdetails['last_name'] = pd_paperdetails['author'].str.split(None, 2).str[-1]\n",
    "    return pd_paperdetails\n",
    "\n",
    "def makedf_personal(personal_details):\n",
    "    # Make DF\n",
    "    df_personal = pd.DataFrame.from_records([personal_details])\n",
    "    # Standardise column names\n",
    "    df_personal = standardise_column_names(df_personal)\n",
    "    return df_personal\n",
    "\n",
    "def reconcile_first_name(df_paper, df_personal):\n",
    "    df_paper = df_paper.merge(df_personal[['first_name','last_name']], on=['last_name'], how='left')\n",
    "    df_paper['first_name'] = np.where(df_paper['first_name_y'].notnull(), df_paper['first_name_y'], df_paper['first_name_x'])\n",
    "    df_paper = df_paper.drop(['first_name_x', 'first_name_y'], axis=1)\n",
    "    df_paper = df_paper.drop_duplicates(subset=['paper','first_name','last_name'])\n",
    "    return df_paper\n",
    "\n",
    "def pipeline_scrape_economists(url):\n",
    "    # Takes an url and output a personal detail and paper dataframes\n",
    "    soup = setup_soup(url)\n",
    "    paper_details = scrape_papers(soup)\n",
    "    personal_details = scrape_personal(soup)\n",
    "    df_paper = makedf_paper(paper_details)\n",
    "    df_personal = makedf_personal(personal_details)\n",
    "    df_paper = reconcile_first_name(df_paper, df_personal)\n",
    "\n",
    "    # Save the url into the author table\n",
    "    url = url.replace('https://ideas.repec.org/', '')\n",
    "    df_personal['author_url'] = url\n",
    "\n",
    "    return df_personal, df_paper\n",
    "\n",
    "def scrape_abstract(df_paper):\n",
    "    paper_urls = df_paper['paper_url'].drop_duplicates()\n",
    "    abstract_dict = {}\n",
    "    for a in tqdm(paper_urls):\n",
    "        try:\n",
    "            soup =  setup_soup('https://ideas.repec.org' + a)\n",
    "            abstract_text = soup.find('div', {'id':'abstract-body'}).text\n",
    "            abstract_dict[a] = abstract_text\n",
    "        except:\n",
    "            print('Paper {} cannot find abstract'.format(a))\n",
    "            abstract_dict[a] = None\n",
    "    abstract_table = pd.DataFrame([(x,y) for x,y in abstract_dict.items()], columns = ['paper_url','abstract'])\n",
    "    return abstract_table\n",
    "\n",
    "\n",
    "# url = 'https://ideas.repec.org/e/pag127.html'\n",
    "# soup = setup_soup(url)\n",
    "# paper_details = scrape_papers(soup)\n",
    "# personal_details = scrape_personal(soup)\n",
    "# df_paper = makedf_paper(paper_details)\n",
    "# df_personal = makedf_personal(personal_details)\n",
    "# df_paper = reconcile_first_name(df_paper, df_personal)\n",
    "\n",
    "# ISSUES\n",
    "   # Remove potential duplicates\n",
    "    # df_paper = df_paper.merge(abstract_table, on='paper_url', how='left')\n",
    "    # abstract_table = abstract_table.drop_duplicates(subset=['abstract'])\n",
    "    # df_paper = df_paper[df_paper['paper_url'].isin(list(abstract_table['paper_url'].unique()))]\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading links: 100%|██████████| 61464/61464 [00:00<00:00, 492386.31it/s]\n"
     ]
    }
   ],
   "source": [
    "urls = get_author_urls()['author_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}