{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('zeina': conda)",
   "metadata": {
    "interpreter": {
     "hash": "23145376f35b1170ea1bf4b28bc7e8e771587d1d28226557d53e8705cb8082c2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "homepage not found\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import unicodedata\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ----------- UTILITY ---------------\n",
    "def clean_string(string):\n",
    "    string = unicodedata.normalize('NFKD',string) \\\n",
    "        .encode('ascii', 'ignore') \\\n",
    "        .decode('ascii') \\\n",
    "        .lower() \\\n",
    "        .strip() \\\n",
    "        .title()\n",
    "    return string\n",
    "\n",
    "def clean_series(series):\n",
    "    cleaned = series.map(clean_string)\n",
    "    return cleaned\n",
    "\n",
    "# PIPELINE ----------------->\n",
    "# Set up soup\n",
    "def setup_soup(url):\n",
    "    # Setup beautiful soup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Scrape papers information from the author\n",
    "def scrape_papers(soup):\n",
    "    # Publication classifications free, gate, none\n",
    "    # There will be overlaps\n",
    "    publications = soup.find_all('li', class_={'list-group-item downfree', \\\n",
    "        'list-group-item downgate', 'list-group-item downnone'})\n",
    "    paper_details = {}\n",
    "\n",
    "    i = 1\n",
    "    for pub in publications:\n",
    "        try:\n",
    "            title = pub.find('a').text\n",
    "            name_year = pub.text.strip().split('\\n')[0]\n",
    "            if 'undated' in name_year:\n",
    "                year = None\n",
    "                authors = re.sub(r', \\\"undated\\\"', '',name_year).split(' & ')\n",
    "            else:\n",
    "                year = re.findall(r', (\\d{4})\\.', name_year)[0]\n",
    "                authors = re.sub(r', \\d{4}\\.', '',name_year).split(' & ')\n",
    "            paper_details[title] = {'author': authors, 'year': year}\n",
    "        except:\n",
    "            print('something went wrong at paper {}'.format(i))\n",
    "        i +=1\n",
    "    return paper_details\n",
    "\n",
    "# Scraping personal information of the author\n",
    "def scrape_personal(soup):\n",
    "    # Find portion where personal details lie in\n",
    "    personal_details = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    # Set up a dictionary to collect all personal information\n",
    "    per = {}\n",
    "    for p in personal_details:\n",
    "        k = p.find_all('td')[0].text.replace(':','')\n",
    "        v = p.find_all('td')[1].text.strip()\n",
    "        per[k] = v\n",
    "    \n",
    "    per_clean = {k:v for (k,v) in per.items() if (v is not '') }\n",
    "    \n",
    "\n",
    "    # Find homepage link\n",
    "    try:    \n",
    "        homepage = soup.find('td', {'class':'homelabel'}).next_sibling.find('a', href=True)['href']\n",
    "        per_clean['Homepage'] = homepage\n",
    "    except:\n",
    "        print('homepage not found')\n",
    "\n",
    "    # Find affiliation - can have multiple\n",
    "    affiliation_soup = soup.find('div', {'id':'affiliation'})\n",
    "\n",
    "    i = 0\n",
    "    try:\n",
    "        for a in affiliation_soup.find_all('h3'):\n",
    "            if a.find('br'):\n",
    "                department = a.find('br').previous_sibling\n",
    "                organisation = a.find('br').next_sibling\n",
    "            else:\n",
    "                print('no breaks in affiliation')\n",
    "                department = ''\n",
    "                organisation = a\n",
    "            per_clean['Aff_Department{}'.format(i)] = department\n",
    "            per_clean['Aff_Organisation{}'.format(i)] = organisation\n",
    "            i += 1\n",
    "    except:\n",
    "        print('affiliation not found')\n",
    "\n",
    "    # Find affiliation locations - can have multiple\n",
    "    i = 0\n",
    "    try:\n",
    "        for a in affiliation_soup.find_all('span', {'class':'locationlabel'}):\n",
    "            if a:\n",
    "                location = a.text\n",
    "            else:\n",
    "                print('no location in affiliation')\n",
    "            per_clean['Aff_Location{}'.format(i)] = location\n",
    "            i += 1\n",
    "    except:\n",
    "        print('affiliation not found')\n",
    "\n",
    "    # Drop unnamed items\n",
    "    per_clean = {k:v for (k,v) in per_clean.items() if (k is not '') }\n",
    "\n",
    "    return per_clean\n",
    "\n",
    "# Flatten the paper details into a dataframe to be inserted into database\n",
    "def makedf_paper(paper_details):\n",
    "    # Flatten the paper_details dictionary into a pandas dataframe\n",
    "    pd_paperdetails = pd.DataFrame(paper_details) \\\n",
    "        .transpose() \\\n",
    "        .explode('author') \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns = {'index':'paper'})\n",
    "    \n",
    "    # Make capitalise titles\n",
    "    pd_paperdetails[['paper','author']] = pd_paperdetails[['paper','author']] \\\n",
    "        .apply(clean_series, axis=1)\n",
    "\n",
    "    # Drop duplicates\n",
    "    pd_paperdetails = pd_paperdetails.drop_duplicates(\n",
    "        subset = ['paper', 'author'])\n",
    "    \n",
    "    # Drop titles that are very similar\n",
    "    similar = process.dedupe(list(pd_paperdetails['paper'].unique()), threshold = 95)\n",
    "    pd_paperdetails = pd_paperdetails[pd_paperdetails['paper'].isin(similar)]\n",
    "   \n",
    "   # Convert \n",
    "    return pd_paperdetails\n",
    "\n",
    "url = 'https://ideas.repec.org/e/pag127.html'\n",
    "soup = setup_soup(url)\n",
    "paper_details = scrape_papers(soup)\n",
    "personal_details = scrape_personal(soup)\n",
    "df_paper = makedf_paper(paper_details)\n",
    "df_personal = pd.DataFrame.from_records([personal_details])\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_table = df_paper[['paper','year']].drop_duplicates().reset_index(drop=True)\n",
    "paper_table['paper_id'] = np.arange(paper_table.shape[0]) # Dependent on max number from database\n",
    "paper_table['year'] = pd.to_numeric(paper_table['year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def standardise_column_names(df, remove_punct=True):\n",
    "    \"\"\" Converts all DataFrame column names to lower case replacing\n",
    "    whitespace of any length with a single underscore. Can also strip\n",
    "    all punctuation from column names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame with non-standardised column names.\n",
    "    remove_punct: bool (default True)\n",
    "        If True will remove all punctuation from column names.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame with standardised column names.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "    for c in df.columns:\n",
    "        c_mod = c.lower()\n",
    "        if remove_punct:            \n",
    "            c_mod = c_mod.translate(translator)\n",
    "        c_mod = '_'.join(c_mod.split(' '))\n",
    "        if c_mod[-1] == '_':\n",
    "            c_mod = c_mod[:-1]\n",
    "        c_mod = re.sub(r'\\_+', '_', c_mod)\n",
    "        df.rename({c: c_mod}, inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data cleaning\n",
    "# Standardise column names\n",
    "df_personal = standardise_column_names(df_personal)\n",
    "# Replace anything like (Ed.)\n",
    "df_paper['author'] = df_paper['author'].str.replace(r'\\(.*\\)', '')\n",
    "df_paper['year'] = pd.to_numeric(paper_table['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_table = df_paper[['author']].drop_duplicates().reset_index(drop=True)\n",
    "author_table['author_id'] = np.arange(author_table.shape[0])\n",
    "author_table['first_name'] = author_table['author'].str.split().str[0]\n",
    "author_table['last_name'] = author_table['author'].str.split().str[-1]\n",
    "author_table = author_table.merge(df_personal, on=['first_name','last_name'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_author_table = df_paper[['paper','author']].merge(author_table[['author','author_id']], on=['author'], how='left') \\\n",
    "    .merge(paper_table[['paper','paper_id']], on=['paper'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_author_table = paper_author_table[['author_id','paper_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "paper        object\n",
       "year        float64\n",
       "paper_id      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "paper_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                paper                author  \\\n",
       "0   Do Merger Policies Increase University Efficie...      Tommaso Agasisti   \n",
       "1   Do Merger Policies Increase University Efficie...        Aleksei Egorov   \n",
       "2   Do Merger Policies Increase University Efficie...    Margarita Maximova   \n",
       "3   Local Governments Efficiency And Educational R...        Simona Ferraro   \n",
       "4   Local Governments Efficiency And Educational R...      Tommaso Agasisti   \n",
       "5   Local Governments Efficiency And Educational R...    Francesco Porcelli   \n",
       "6   Local Governments Efficiency And Educational R...           Mara Soncin   \n",
       "7   Public Finance, Government Spending And Econom...      Tommaso Agasisti   \n",
       "8   Public Finance, Government Spending And Econom...        Cristian Barra   \n",
       "9   Public Finance, Government Spending And Econom...         Roberto Zotti   \n",
       "10  Public Finance, Government Spending And Econom...     Agasisti, Tommaso   \n",
       "11  Public Finance, Government Spending And Econom...        Barra,Cristian   \n",
       "12  Public Finance, Government Spending And Econom...        Zotti, Roberto   \n",
       "13  Autonomy, Performance And Efficiency: An Empir...      Tommaso Agasisti   \n",
       "14  Autonomy, Performance And Efficiency: An Empir...   Ekaterina Shibanova   \n",
       "15  How Do The Characteristics Of The Environment ...      Tommaso Agasisti   \n",
       "16  How Do The Characteristics Of The Environment ...        Aleksei Egorov   \n",
       "17  How Do The Characteristics Of The Environment ...   Pavel Serebrennikov   \n",
       "18  The Causal Impact Of Performance-Based Funding...      Tommaso Agasisti   \n",
       "19  The Causal Impact Of Performance-Based Funding...  Ekaterina Abalmasova   \n",
       "20  The Causal Impact Of Performance-Based Funding...   Ekaterina Shibanova   \n",
       "21  The Causal Impact Of Performance-Based Funding...        Aleksei Egorov   \n",
       "22  The Added Value Of More Accurate Predictions F...         Fritz Schiltz   \n",
       "23  The Added Value Of More Accurate Predictions F...         Paolo Sestito   \n",
       "24  The Added Value Of More Accurate Predictions F...      Tommaso Agasisti   \n",
       "25  The Added Value Of More Accurate Predictions F...      Kristof De Witte   \n",
       "26  Local Governments Efficiency And Its Heterogen...      Tommaso Agasisti   \n",
       "27  Local Governments Efficiency And Its Heterogen...    Francesco Porcelli   \n",
       "28  Academic Resilience: What Schools And Countrie...      Tommaso Agasisti   \n",
       "29  Academic Resilience: What Schools And Countrie...    Francesco Avvisati   \n",
       "30  Academic Resilience: What Schools And Countrie...   Francesca Borgonovi   \n",
       "31  Academic Resilience: What Schools And Countrie...     Sergio Longobardi   \n",
       "32  The Russian Excellence Initiative For Higher E...      Tommaso Agasisti   \n",
       "33  The Russian Excellence Initiative For Higher E...   Ekaterina Shibanova   \n",
       "34  The Russian Excellence Initiative For Higher E...       Daria Platonova   \n",
       "35  The Russian Excellence Initiative For Higher E...     Mikhail Lisyutkin   \n",
       "36  Evaluating The Stability Of School Performance...      Tommaso Agasisti   \n",
       "37  Evaluating The Stability Of School Performance...       Veronica Minaya   \n",
       "38  Multidimensional Poverty Measures For Analysin...      Tommaso Agasisti   \n",
       "39  Multidimensional Poverty Measures For Analysin...     Sergio Longobardi   \n",
       "40  Multidimensional Poverty Measures For Analysin...        Vincenzo Prete   \n",
       "\n",
       "      year  \n",
       "0   2020.0  \n",
       "1   2020.0  \n",
       "2   2019.0  \n",
       "3   2019.0  \n",
       "4   2018.0  \n",
       "5   2017.0  \n",
       "6   2019.0  \n",
       "7   2017.0  \n",
       "8   2017.0  \n",
       "9   2020.0  \n",
       "10  2017.0  \n",
       "11  2016.0  \n",
       "12  2015.0  \n",
       "13  2015.0  \n",
       "14  2014.0  \n",
       "15  2013.0  \n",
       "16  2012.0  \n",
       "17  2009.0  \n",
       "18  2020.0  \n",
       "19  2019.0  \n",
       "20  2019.0  \n",
       "21  2018.0  \n",
       "22  2018.0  \n",
       "23  2017.0  \n",
       "24  2017.0  \n",
       "25  2016.0  \n",
       "26  2016.0  \n",
       "27  2015.0  \n",
       "28  2014.0  \n",
       "29  2013.0  \n",
       "30  2012.0  \n",
       "31  2011.0  \n",
       "32  2011.0  \n",
       "33  2010.0  \n",
       "34  2010.0  \n",
       "35  2009.0  \n",
       "36  2008.0  \n",
       "37  2009.0  \n",
       "38  2017.0  \n",
       "39  2011.0  \n",
       "40  2017.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper</th>\n      <th>author</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do Merger Policies Increase University Efficie...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2020.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Do Merger Policies Increase University Efficie...</td>\n      <td>Aleksei Egorov</td>\n      <td>2020.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Do Merger Policies Increase University Efficie...</td>\n      <td>Margarita Maximova</td>\n      <td>2019.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Local Governments Efficiency And Educational R...</td>\n      <td>Simona Ferraro</td>\n      <td>2019.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Local Governments Efficiency And Educational R...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2018.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Local Governments Efficiency And Educational R...</td>\n      <td>Francesco Porcelli</td>\n      <td>2017.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Local Governments Efficiency And Educational R...</td>\n      <td>Mara Soncin</td>\n      <td>2019.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Public Finance, Government Spending And Econom...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2017.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Public Finance, Government Spending And Econom...</td>\n      <td>Cristian Barra</td>\n      <td>2017.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Public Finance, Government Spending And Econom...</td>\n      <td>Roberto Zotti</td>\n      <td>2020.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Public Finance, Government Spending And Econom...</td>\n      <td>Agasisti, Tommaso</td>\n      <td>2017.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Public Finance, Government Spending And Econom...</td>\n      <td>Barra,Cristian</td>\n      <td>2016.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Public Finance, Government Spending And Econom...</td>\n      <td>Zotti, Roberto</td>\n      <td>2015.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Autonomy, Performance And Efficiency: An Empir...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2015.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Autonomy, Performance And Efficiency: An Empir...</td>\n      <td>Ekaterina Shibanova</td>\n      <td>2014.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>How Do The Characteristics Of The Environment ...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>How Do The Characteristics Of The Environment ...</td>\n      <td>Aleksei Egorov</td>\n      <td>2012.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>How Do The Characteristics Of The Environment ...</td>\n      <td>Pavel Serebrennikov</td>\n      <td>2009.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>The Causal Impact Of Performance-Based Funding...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2020.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>The Causal Impact Of Performance-Based Funding...</td>\n      <td>Ekaterina Abalmasova</td>\n      <td>2019.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>The Causal Impact Of Performance-Based Funding...</td>\n      <td>Ekaterina Shibanova</td>\n      <td>2019.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>The Causal Impact Of Performance-Based Funding...</td>\n      <td>Aleksei Egorov</td>\n      <td>2018.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>The Added Value Of More Accurate Predictions F...</td>\n      <td>Fritz Schiltz</td>\n      <td>2018.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>The Added Value Of More Accurate Predictions F...</td>\n      <td>Paolo Sestito</td>\n      <td>2017.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>The Added Value Of More Accurate Predictions F...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2017.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>The Added Value Of More Accurate Predictions F...</td>\n      <td>Kristof De Witte</td>\n      <td>2016.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Local Governments Efficiency And Its Heterogen...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2016.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Local Governments Efficiency And Its Heterogen...</td>\n      <td>Francesco Porcelli</td>\n      <td>2015.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Academic Resilience: What Schools And Countrie...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2014.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Academic Resilience: What Schools And Countrie...</td>\n      <td>Francesco Avvisati</td>\n      <td>2013.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Academic Resilience: What Schools And Countrie...</td>\n      <td>Francesca Borgonovi</td>\n      <td>2012.0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Academic Resilience: What Schools And Countrie...</td>\n      <td>Sergio Longobardi</td>\n      <td>2011.0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>The Russian Excellence Initiative For Higher E...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2011.0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>The Russian Excellence Initiative For Higher E...</td>\n      <td>Ekaterina Shibanova</td>\n      <td>2010.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>The Russian Excellence Initiative For Higher E...</td>\n      <td>Daria Platonova</td>\n      <td>2010.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>The Russian Excellence Initiative For Higher E...</td>\n      <td>Mikhail Lisyutkin</td>\n      <td>2009.0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Evaluating The Stability Of School Performance...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2008.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Evaluating The Stability Of School Performance...</td>\n      <td>Veronica Minaya</td>\n      <td>2009.0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Multidimensional Poverty Measures For Analysin...</td>\n      <td>Tommaso Agasisti</td>\n      <td>2017.0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Multidimensional Poverty Measures For Analysin...</td>\n      <td>Sergio Longobardi</td>\n      <td>2011.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Multidimensional Poverty Measures For Analysin...</td>\n      <td>Vincenzo Prete</td>\n      <td>2017.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "df_paper[df_paper['year'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}